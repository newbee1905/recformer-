name: gpt2_8l_768d_qknorm_soft
block_size: 1024
num_recurrences: 1
n_layer: 8
n_head: 12
d_model: 768
dropout: 0.1
use_kv_cache: false
use_gate: false
use_qk_norm: true
qk_norm:
  type: soft_hyperball_norm
ffn_multiplier: 4.0
rope_theta: 10000.0
layer_scale_init: 0.0001
tie_word_embeddings: true
use_liger_ff: false
use_liger_norm: false
use_liger_rope: false
