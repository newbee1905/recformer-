defaults:
  - model: recurrent_small
  - _self_

# hydra configuration
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

wandb:
  project: "recurrent_transformer"
  name: null

model:
  max_length: ${model.block_size}
  tokenizer_path: "${hydra:runtime.cwd}/fineweb-edu-tokenizer-vocab-16384"

dataset:
  path: "${hydra:runtime.cwd}/offline_cache/fineweb-edu"

training:
  batch_size: 32
  num_workers: 8
  epochs: 1
  optimizer:
    lr: 0.0001
    weight_decay: 0.01
  logger: "tensorboard" # "tensorboard" or "wandb"
  compile: false
  dtype: "bfloat16" # or "float16" or "float32"
  grad_accum_steps: 2
  grad_clip: 1.0
  early_stopping_patience: 3
  softcap:
    enabled: false
    value: 30.0
